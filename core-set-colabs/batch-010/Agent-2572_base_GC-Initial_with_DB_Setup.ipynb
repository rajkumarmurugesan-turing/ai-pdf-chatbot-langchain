{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ad81679f",
      "metadata": {
        "id": "ad81679f"
      },
      "source": [
        "**Sample ID:** 2572\n",
        "\n",
        "**Query:**\n",
        "\n",
        "\n",
        "Are there accounts with an initial credit balance?\n",
        "\n",
        "**DB Type:** Base Case\n",
        "\n",
        "**Case Description:**\n",
        "\n",
        "The dataset `2_Debits_and_Credits_Purchases.csv` exists in datasets and has md5Checksum `b19ce41e7c74cd9de6fc95a882803ab4` .\n",
        "\n",
        "```\n",
        "<additional_data>\n",
        "<current_file>Path:Datasets/2_Debits_and_Credits_Purchases.csv</current_file>\n",
        "</additional_data>\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "**Global/Context Variables:**\n",
        "\n",
        "**APIs:**\n",
        "- terminal"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6b2bf66",
      "metadata": {
        "id": "b6b2bf66"
      },
      "source": [
        "# Set Up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "518d04ae",
      "metadata": {
        "id": "518d04ae"
      },
      "source": [
        "## Download relevant files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e77c368",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e77c368",
        "outputId": "a66717fa-e48b-4316-b46c-242d099d963e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Searching for APIs zip file with version 0.0.6 in folder: 1QpkAZxXhVFzIbm8qPGPRP1YqXEvJ4uD4...\n",
            "Found matching file: APIs_V0.0.6.zip (ID: 1LZ9uyD9xU9U32zr4fztMXiWmIhfJtdj-)\n",
            "Downloading APIs zip file with ID: 1LZ9uyD9xU9U32zr4fztMXiWmIhfJtdj-...\n",
            "Download progress: 100%\n",
            "Extracting specific items from /content/APIs_V0.0.6.zip to /content...\n",
            "\n",
            "Verifying extracted items:\n",
            "✅ /content/APIs is present.\n",
            "✅ /content/DBs is present.\n",
            "✅ /content/Scripts is present.\n",
            "\n",
            "✅ Setup complete! Required items extracted to /content.\n",
            "\n",
            "Generating FC Schemas\n",
            "Successfully loaded _function_map via AST for package 'mongodb'.\n",
            "17 functions targeted for schema generation.\n",
            "Schema generated for package 'mongodb' (as JSON array) and saved to /content/Schemas/mongodb.json\n",
            "Successfully loaded _function_map via AST for package 'sapconcur'.\n",
            "14 functions targeted for schema generation.\n",
            "Schema generated for package 'sapconcur' (as JSON array) and saved to /content/Schemas/sapconcur.json\n",
            "Successfully loaded _function_map via AST for package 'jira'.\n",
            "88 functions targeted for schema generation.\n",
            "Schema generated for package 'jira' (as JSON array) and saved to /content/Schemas/jira.json\n",
            "Successfully loaded _function_map via AST for package 'stripe'.\n",
            "20 functions targeted for schema generation.\n",
            "Schema generated for package 'stripe' (as JSON array) and saved to /content/Schemas/stripe.json\n",
            "Successfully loaded _function_map via AST for package 'gdrive'.\n",
            "44 functions targeted for schema generation.\n",
            "Schema generated for package 'gdrive' (as JSON array) and saved to /content/Schemas/gdrive.json\n",
            "Successfully loaded _function_map via AST for package 'bigquery'.\n",
            "3 functions targeted for schema generation.\n",
            "Schema generated for package 'bigquery' (as JSON array) and saved to /content/Schemas/bigquery.json\n",
            "Successfully loaded _function_map via AST for package 'sdm'.\n",
            "5 functions targeted for schema generation.\n",
            "Schema generated for package 'sdm' (as JSON array) and saved to /content/Schemas/sdm.json\n",
            "Successfully loaded _function_map via AST for package 'youtube'.\n",
            "43 functions targeted for schema generation.\n",
            "Schema generated for package 'youtube' (as JSON array) and saved to /content/Schemas/youtube.json\n",
            "Successfully loaded _function_map via AST for package 'mysql'.\n",
            "3 functions targeted for schema generation.\n",
            "Schema generated for package 'mysql' (as JSON array) and saved to /content/Schemas/mysql.json\n",
            "Successfully loaded _function_map via AST for package 'slack'.\n",
            "63 functions targeted for schema generation.\n",
            "Schema generated for package 'slack' (as JSON array) and saved to /content/Schemas/slack.json\n",
            "Successfully loaded _function_map via AST for package 'figma'.\n",
            "22 functions targeted for schema generation.\n",
            "Schema generated for package 'figma' (as JSON array) and saved to /content/Schemas/figma.json\n",
            "Successfully loaded _function_map via AST for package 'linkedin'.\n",
            "18 functions targeted for schema generation.\n",
            "Schema generated for package 'linkedin' (as JSON array) and saved to /content/Schemas/linkedin.json\n",
            "Successfully loaded _function_map via AST for package 'google_chat'.\n",
            "31 functions targeted for schema generation.\n",
            "Schema generated for package 'google_chat' (as JSON array) and saved to /content/Schemas/google_chat.json\n",
            "Successfully loaded _function_map via AST for package 'google_cloud_storage'.\n",
            "14 functions targeted for schema generation.\n",
            "Schema generated for package 'google_cloud_storage' (as JSON array) and saved to /content/Schemas/google_cloud_storage.json\n",
            "Successfully loaded _function_map via AST for package 'github'.\n",
            "32 functions targeted for schema generation.\n",
            "Schema generated for package 'github' (as JSON array) and saved to /content/Schemas/github.json\n",
            "Successfully loaded _function_map via AST for package 'blender'.\n",
            "13 functions targeted for schema generation.\n",
            "Schema generated for package 'blender' (as JSON array) and saved to /content/Schemas/blender.json\n",
            "Successfully loaded _function_map via AST for package 'azure'.\n",
            "24 functions targeted for schema generation.\n",
            "Schema generated for package 'azure' (as JSON array) and saved to /content/Schemas/azure.json\n",
            "Successfully loaded _function_map via AST for package 'tiktok'.\n",
            "3 functions targeted for schema generation.\n",
            "Schema generated for package 'tiktok' (as JSON array) and saved to /content/Schemas/tiktok.json\n",
            "Successfully loaded _function_map via AST for package 'instagram'.\n",
            "10 functions targeted for schema generation.\n",
            "Schema generated for package 'instagram' (as JSON array) and saved to /content/Schemas/instagram.json\n",
            "Successfully loaded _function_map via AST for package 'google_docs'.\n",
            "3 functions targeted for schema generation.\n",
            "Schema generated for package 'google_docs' (as JSON array) and saved to /content/Schemas/google_docs.json\n",
            "Successfully loaded _function_map via AST for package 'shopify'.\n",
            "25 functions targeted for schema generation.\n",
            "Schema generated for package 'shopify' (as JSON array) and saved to /content/Schemas/shopify.json\n",
            "Successfully loaded _function_map via AST for package 'hubspot'.\n",
            "37 functions targeted for schema generation.\n",
            "Schema generated for package 'hubspot' (as JSON array) and saved to /content/Schemas/hubspot.json\n",
            "Successfully loaded _function_map via AST for package 'cursor'.\n",
            "15 functions targeted for schema generation.\n",
            "Schema generated for package 'cursor' (as JSON array) and saved to /content/Schemas/cursor.json\n",
            "Successfully loaded _function_map via AST for package 'supabase'.\n",
            "26 functions targeted for schema generation.\n",
            "Schema generated for package 'supabase' (as JSON array) and saved to /content/Schemas/supabase.json\n",
            "Successfully loaded _function_map via AST for package 'puppeteer'.\n",
            "5 functions targeted for schema generation.\n",
            "Schema generated for package 'puppeteer' (as JSON array) and saved to /content/Schemas/puppeteer.json\n",
            "Successfully loaded _function_map via AST for package 'google_sheets'.\n",
            "15 functions targeted for schema generation.\n",
            "Schema generated for package 'google_sheets' (as JSON array) and saved to /content/Schemas/google_sheets.json\n",
            "Successfully loaded _function_map via AST for package 'home_assistant'.\n",
            "6 functions targeted for schema generation.\n",
            "Schema generated for package 'home_assistant' (as JSON array) and saved to /content/Schemas/home_assistant.json\n",
            "Successfully loaded _function_map via AST for package 'confluence'.\n",
            "39 functions targeted for schema generation.\n",
            "Schema generated for package 'confluence' (as JSON array) and saved to /content/Schemas/confluence.json\n",
            "Successfully loaded _function_map via AST for package 'salesforce'.\n",
            "26 functions targeted for schema generation.\n",
            "Schema generated for package 'salesforce' (as JSON array) and saved to /content/Schemas/salesforce.json\n",
            "Successfully loaded _function_map via AST for package 'google_slides'.\n",
            "5 functions targeted for schema generation.\n",
            "Schema generated for package 'google_slides' (as JSON array) and saved to /content/Schemas/google_slides.json\n",
            "Successfully loaded _function_map via AST for package 'gmail'.\n",
            "60 functions targeted for schema generation.\n",
            "Schema generated for package 'gmail' (as JSON array) and saved to /content/Schemas/gmail.json\n",
            "Successfully loaded _function_map via AST for package 'reddit'.\n",
            "247 functions targeted for schema generation.\n",
            "Schema generated for package 'reddit' (as JSON array) and saved to /content/Schemas/reddit.json\n",
            "Successfully loaded _function_map via AST for package 'terminal'.\n",
            "1 functions targeted for schema generation.\n",
            "Schema generated for package 'terminal' (as JSON array) and saved to /content/Schemas/terminal.json\n",
            "Successfully loaded _function_map via AST for package 'workday'.\n",
            "191 functions targeted for schema generation.\n",
            "Schema generated for package 'workday' (as JSON array) and saved to /content/Schemas/workday.json\n",
            "Successfully loaded _function_map via AST for package 'copilot'.\n",
            "17 functions targeted for schema generation.\n",
            "Schema generated for package 'copilot' (as JSON array) and saved to /content/Schemas/copilot.json\n",
            "Successfully loaded _function_map via AST for package 'google_meet'.\n",
            "16 functions targeted for schema generation.\n",
            "Schema generated for package 'google_meet' (as JSON array) and saved to /content/Schemas/google_meet.json\n",
            "Successfully loaded _function_map via AST for package 'canva'.\n",
            "32 functions targeted for schema generation.\n",
            "Schema generated for package 'canva' (as JSON array) and saved to /content/Schemas/canva.json\n",
            "Successfully loaded _function_map via AST for package 'google_maps'.\n",
            "5 functions targeted for schema generation.\n",
            "Schema generated for package 'google_maps' (as JSON array) and saved to /content/Schemas/google_maps.json\n",
            "Successfully loaded _function_map via AST for package 'github_actions'.\n",
            "9 functions targeted for schema generation.\n",
            "Schema generated for package 'github_actions' (as JSON array) and saved to /content/Schemas/github_actions.json\n",
            "Successfully loaded _function_map via AST for package 'zendesk'.\n",
            "15 functions targeted for schema generation.\n",
            "Schema generated for package 'zendesk' (as JSON array) and saved to /content/Schemas/zendesk.json\n",
            "Successfully loaded _function_map via AST for package 'whatsapp'.\n",
            "12 functions targeted for schema generation.\n",
            "Schema generated for package 'whatsapp' (as JSON array) and saved to /content/Schemas/whatsapp.json\n",
            "Successfully loaded _function_map via AST for package 'google_calendar'.\n",
            "33 functions targeted for schema generation.\n",
            "Schema generated for package 'google_calendar' (as JSON array) and saved to /content/Schemas/google_calendar.json\n",
            "✅ Successfully generated 42 FC Schemas to /content/Schemas\n",
            "Starting download of folder 1tZqZB1vAxp4TTxbPm6O2YjfkZD4FM-ml to ./workspace/Datasets...\n",
            "Creating subfolder and downloading: ./workspace/Datasets/Whole set\n",
            "Downloading file: Quotewk.csv to ./workspace/Datasets/Quotewk.csv\n",
            "Downloading file: ICEQueries_Files_md5Checksum.csv to ./workspace/Datasets/ICEQueries_Files_md5Checksum.csv\n",
            "Downloading file: torque and flow performance.csv to ./workspace/Datasets/torque and flow performance.csv\n",
            "Downloading file: eBay-ListingsSalesReport-Dec-22-2023-08_21_39-0700-12145337749 - Karina Arango.csv to ./workspace/Datasets/eBay-ListingsSalesReport-Dec-22-2023-08_21_39-0700-12145337749 - Karina Arango.csv\n",
            "Downloading file: visits_2023-11-01 - Spencer Miller.csv to ./workspace/Datasets/visits_2023-11-01 - Spencer Miller.csv\n",
            "Downloading file: CLIENTS AB USD_CLEAN.csv to ./workspace/Datasets/CLIENTS AB USD_CLEAN.csv\n",
            "Downloading file: Sales_Report.csv to ./workspace/Datasets/Sales_Report.csv\n",
            "Downloading file: Truck Repairs for 2023 - Sheet1 - Dr. Victoria Gamble.csv to ./workspace/Datasets/Truck Repairs for 2023 - Sheet1 - Dr. Victoria Gamble.csv\n",
            "Downloading file: prodqualityanalysis.csv to ./workspace/Datasets/prodqualityanalysis.csv\n",
            "Downloading file: GARDEN LOG - Visits (1) - Erica Redling.csv to ./workspace/Datasets/GARDEN LOG - Visits (1) - Erica Redling.csv\n",
            "Downloading file: business users.csv to ./workspace/Datasets/business users.csv\n",
            "Downloading file: Earnings And Placements - statement.csv - Jonathan Foster.csv to ./workspace/Datasets/Earnings And Placements - statement.csv - Jonathan Foster.csv\n",
            "Downloading file: Clockify Detailed Time Report - 2023.csv to ./workspace/Datasets/Clockify Detailed Time Report - 2023.csv\n",
            "Downloading file: Retrogasm sales 2023 1 - Rachel Robinson.csv to ./workspace/Datasets/Retrogasm sales 2023 1 - Rachel Robinson.csv\n",
            "Downloading file: Construction_costs_delivery.csv to ./workspace/Datasets/Construction_costs_delivery.csv\n",
            "Downloading file: Expenses_20152.csv to ./workspace/Datasets/Expenses_20152.csv\n",
            "Downloading file: Vat_sales_ledger 02-2023.csv to ./workspace/Datasets/Vat_sales_ledger 02-2023.csv\n",
            "Downloading file: Salary and Equity Data.csv to ./workspace/Datasets/Salary and Equity Data.csv\n",
            "Downloading file: 2020 sales - Shannon O.csv to ./workspace/Datasets/2020 sales - Shannon O.csv\n",
            "Downloading file: Comparison with Wal Mart Catalog Stock.csv to ./workspace/Datasets/Comparison with Wal Mart Catalog Stock.csv\n",
            "Downloading file: burbujas_sales_july.csv to ./workspace/Datasets/burbujas_sales_july.csv\n",
            "Downloading file: Wood purchased - Patrick Bernier.csv to ./workspace/Datasets/Wood purchased - Patrick Bernier.csv\n",
            "Downloading file: Dairy Solids (fat+protein).csv to ./workspace/Datasets/Dairy Solids (fat+protein).csv\n",
            "Downloading file: Prices July 2022 EQUILIBRIO.csv to ./workspace/Datasets/Prices July 2022 EQUILIBRIO.csv\n",
            "Downloading file: payroll_sheet.csv to ./workspace/Datasets/payroll_sheet.csv\n",
            "Downloading file: [External] order_items.csv to ./workspace/Datasets/[External] order_items.csv\n",
            "Downloading file: orders - orders - Kimm Topping.csv to ./workspace/Datasets/orders - orders - Kimm Topping.csv\n",
            "Downloading file: 2_Debits_and_Credits_Purchases.csv to ./workspace/Datasets/2_Debits_and_Credits_Purchases.csv\n",
            "Downloading file: Content Delivery Data Set - Jacob Goldberg.csv to ./workspace/Datasets/Content Delivery Data Set - Jacob Goldberg.csv\n",
            "Downloading file: Purchase_invoice_graphics_company.csv to ./workspace/Datasets/Purchase_invoice_graphics_company.csv\n",
            "Downloading file: Accounting - Jake Chase.csv to ./workspace/Datasets/Accounting - Jake Chase.csv\n",
            "Downloading file: Higher Ed Threads 2023 - Phil Nance.csv to ./workspace/Datasets/Higher Ed Threads 2023 - Phil Nance.csv\n",
            "Downloading file: Cars_Sales_22-23.csv to ./workspace/Datasets/Cars_Sales_22-23.csv\n",
            "Downloading file: COMAFI AND SUPER HISTORY.csv to ./workspace/Datasets/COMAFI AND SUPER HISTORY.csv\n",
            "Downloading file: Survey Meters.csv to ./workspace/Datasets/Survey Meters.csv\n",
            "Downloading file: Juana Events Rentals.csv to ./workspace/Datasets/Juana Events Rentals.csv\n",
            "Downloading file: 2201_VoucherCheker.csv to ./workspace/Datasets/2201_VoucherCheker.csv\n",
            "Downloading file: Blueprint_budget.csv to ./workspace/Datasets/Blueprint_budget.csv\n",
            "Downloading file: Storage (version 1).csv - Sheet1 - Erica Redling.csv to ./workspace/Datasets/Storage (version 1).csv - Sheet1 - Erica Redling.csv\n",
            "Downloading file: Productivity2.csv to ./workspace/Datasets/Productivity2.csv\n",
            "Downloading file: Woodworking Class Data - Patrick Bernier.csv to ./workspace/Datasets/Woodworking Class Data - Patrick Bernier.csv\n",
            "Downloading file: March Sales.csv to ./workspace/Datasets/March Sales.csv\n",
            "Downloading file: Survey - Videogames - Brazil 2015 - Alexandre Papanis.csv to ./workspace/Datasets/Survey - Videogames - Brazil 2015 - Alexandre Papanis.csv\n",
            "Downloading file: Wine Inventory - Andrew Nelson.csv to ./workspace/Datasets/Wine Inventory - Andrew Nelson.csv\n",
            "Downloading file: Toggl Detailed Time Report - 2022.csv to ./workspace/Datasets/Toggl Detailed Time Report - 2022.csv\n",
            "Downloading file: IllinoisChicago_SuiteRentals_Aug2023 - Dalron J. Robertson.csv to ./workspace/Datasets/IllinoisChicago_SuiteRentals_Aug2023 - Dalron J. Robertson.csv\n",
            "Downloading file: Dataset for Solar Panel Cleaning - Nestor Sanchez.csv to ./workspace/Datasets/Dataset for Solar Panel Cleaning - Nestor Sanchez.csv\n",
            "Downloading file: 2022 Districts Monthly Transfer Dataset.csv to ./workspace/Datasets/2022 Districts Monthly Transfer Dataset.csv\n",
            "Downloading file: balance_sheet.csv to ./workspace/Datasets/balance_sheet.csv\n",
            "Downloading file: Parcels_2023Q1_CO - Guillermo Pardon.csv to ./workspace/Datasets/Parcels_2023Q1_CO - Guillermo Pardon.csv\n",
            "Downloading file: OOC-55 - William Webster (SickBoy).csv to ./workspace/Datasets/OOC-55 - William Webster (SickBoy).csv\n",
            "Downloading file: Materials_CTM.csv to ./workspace/Datasets/Materials_CTM.csv\n",
            "Downloading file: cash_flow_Bakery.csv to ./workspace/Datasets/cash_flow_Bakery.csv\n",
            "Downloading file: chicken_groceries_prices.csv to ./workspace/Datasets/chicken_groceries_prices.csv\n",
            "Downloading file: Data Set Remotasks - Anita Portnoy.csv to ./workspace/Datasets/Data Set Remotasks - Anita Portnoy.csv\n",
            "Downloading file: financial_credits.csv to ./workspace/Datasets/financial_credits.csv\n",
            "Downloading file: Cristine's Corner January - Cristine Marquez.csv to ./workspace/Datasets/Cristine's Corner January - Cristine Marquez.csv\n",
            "Downloading file: Tecno_MP_activities-feeder_collection-20231206183310-efa71.csv to ./workspace/Datasets/Tecno_MP_activities-feeder_collection-20231206183310-efa71.csv\n",
            "Downloading file: airbnb_tax_01_2023-01_2024 - Philip Ferraro.csv to ./workspace/Datasets/airbnb_tax_01_2023-01_2024 - Philip Ferraro.csv\n",
            "Downloading file: Log_2016_2020_Redacted - Adam Johnson.csv to ./workspace/Datasets/Log_2016_2020_Redacted - Adam Johnson.csv\n",
            "Downloading file: U0089_Clients.csv to ./workspace/Datasets/U0089_Clients.csv\n",
            "Downloading file: MarAug2023_ProjInst_Dataset - Diana.csv to ./workspace/Datasets/MarAug2023_ProjInst_Dataset - Diana.csv\n",
            "Downloading file: Vat_purchases_journal_02-2023.csv to ./workspace/Datasets/Vat_purchases_journal_02-2023.csv\n",
            "Downloading file: ServerProcesses.csv to ./workspace/Datasets/ServerProcesses.csv\n",
            "Downloading file: Bold21 Data set - orders_export_1.csv to ./workspace/Datasets/Bold21 Data set - orders_export_1.csv\n",
            "Downloading file: Payroll_oct_16-31.csv to ./workspace/Datasets/Payroll_oct_16-31.csv\n",
            "Downloading file: Expense_summary.csv to ./workspace/Datasets/Expense_summary.csv\n",
            "Downloading file: KookyArtsAi DATABASE - Christy Rivers.csv to ./workspace/Datasets/KookyArtsAi DATABASE - Christy Rivers.csv\n",
            "Downloading file: Sales_RLC2.csv to ./workspace/Datasets/Sales_RLC2.csv\n",
            "Downloading file: Fragrance June.csv to ./workspace/Datasets/Fragrance June.csv\n",
            "Downloading file: Vector360-TravelResponse - Ernesto Herrero.csv to ./workspace/Datasets/Vector360-TravelResponse - Ernesto Herrero.csv\n",
            "Downloading file: sales.csv to ./workspace/Datasets/sales.csv\n",
            "Downloading file: Paper_Pen_Pavilion_Sales_09_12 - Cordejah Walker.csv to ./workspace/Datasets/Paper_Pen_Pavilion_Sales_09_12 - Cordejah Walker.csv\n",
            "Downloading file: EtsySoldOrders2021_2023 - Sara Gerges.csv to ./workspace/Datasets/EtsySoldOrders2021_2023 - Sara Gerges.csv\n",
            "Downloading file: Student supplies delivery costs Fall 2023 - Robert Russell.csv to ./workspace/Datasets/Student supplies delivery costs Fall 2023 - Robert Russell.csv\n",
            "Downloading file: KiltX Web3 Summits Anonymized Data Keri Kilty - keri kilty.csv to ./workspace/Datasets/KiltX Web3 Summits Anonymized Data Keri Kilty - keri kilty.csv\n",
            "Downloading file: BurguerhouseJuly21.csv to ./workspace/Datasets/BurguerhouseJuly21.csv\n",
            "Downloading file: mill_operations.csv to ./workspace/Datasets/mill_operations.csv\n",
            "Downloading file: Store Survey - Patrick Bernier.csv to ./workspace/Datasets/Store Survey - Patrick Bernier.csv\n",
            "Downloading file: Monthly Cash Flow.csv to ./workspace/Datasets/Monthly Cash Flow.csv\n",
            "Downloading file: supplies.csv to ./workspace/Datasets/supplies.csv\n",
            "Downloading file: public_lighting.csv to ./workspace/Datasets/public_lighting.csv\n",
            "Downloading file: rtshare - Kris Best.csv to ./workspace/Datasets/rtshare - Kris Best.csv\n",
            "Downloading file: First School Term 2023_2024 - Robert Russell.csv to ./workspace/Datasets/First School Term 2023_2024 - Robert Russell.csv\n",
            "Downloading file: order.list.export 2023-12-22 (2) - MOZZAM SHAHID.csv to ./workspace/Datasets/order.list.export 2023-12-22 (2) - MOZZAM SHAHID.csv\n",
            "Downloading file: R Plan Budget.csv to ./workspace/Datasets/R Plan Budget.csv\n",
            "Downloading file: Report_periodical_records.csv to ./workspace/Datasets/Report_periodical_records.csv\n",
            "Downloading file: Submission id - Miss Kaye Estacio.csv to ./workspace/Datasets/Submission id - Miss Kaye Estacio.csv\n",
            "Downloading file: Product Dims.csv to ./workspace/Datasets/Product Dims.csv\n",
            "Downloading file: SalesFiguresQ1Q2 - Travis Colahan.csv to ./workspace/Datasets/SalesFiguresQ1Q2 - Travis Colahan.csv\n",
            "Downloading file: Campaigns BIMO.csv to ./workspace/Datasets/Campaigns BIMO.csv\n",
            "Downloading file: prices_ss.csv to ./workspace/Datasets/prices_ss.csv\n",
            "Downloading file: ABM_WAGES.csv to ./workspace/Datasets/ABM_WAGES.csv\n",
            "Downloading file: Bill_A2.csv to ./workspace/Datasets/Bill_A2.csv\n",
            "Downloading file: HEALTHCARE AFFILIATE MANAGEMENT.csv to ./workspace/Datasets/HEALTHCARE AFFILIATE MANAGEMENT.csv\n",
            "Downloading file: Gwynstone.Originz.dat - Gwynne Rife.csv to ./workspace/Datasets/Gwynstone.Originz.dat - Gwynne Rife.csv\n",
            "Downloading file: Purchase_orders_GAP_ZEN.csv to ./workspace/Datasets/Purchase_orders_GAP_ZEN.csv\n",
            "Downloading file: data-export (3) - Harsh Bakshi.csv to ./workspace/Datasets/data-export (3) - Harsh Bakshi.csv\n",
            "Downloading file: Waiters Cash_Sales.csv to ./workspace/Datasets/Waiters Cash_Sales.csv\n",
            "Downloading file: Mama Milk Flow Data - paul miles.csv to ./workspace/Datasets/Mama Milk Flow Data - paul miles.csv\n",
            "Downloading file: JAZ Ceramics.csv to ./workspace/Datasets/JAZ Ceramics.csv\n",
            "Downloading file: AccumulatorReadingsReport.csv to ./workspace/Datasets/AccumulatorReadingsReport.csv\n",
            "Downloading file: Checks.csv to ./workspace/Datasets/Checks.csv\n",
            "Downloading file: Bank_accreditations.csv to ./workspace/Datasets/Bank_accreditations.csv\n",
            "Downloading file: vaccinations_city_flores.csv to ./workspace/Datasets/vaccinations_city_flores.csv\n",
            "Downloading file: Sheet1 - Brooks Chiro Rehab.csv to ./workspace/Datasets/Sheet1 - Brooks Chiro Rehab.csv\n",
            "Downloading file: Values Survey - June 2023.csv to ./workspace/Datasets/Values Survey - June 2023.csv\n",
            "Downloading file: ChemicalProductsApplications.csv to ./workspace/Datasets/ChemicalProductsApplications.csv\n",
            "Downloading file: BurguerhouseJuly21csv.csv to ./workspace/Datasets/BurguerhouseJuly21csv.csv\n",
            "Downloading file: TD_HET Shipping Report - Phil Nance.csv to ./workspace/Datasets/TD_HET Shipping Report - Phil Nance.csv\n",
            "Downloading file: shop_orders_export_shop_id_5874055_from_2023-11-01_to_2023-11-30 - Phil Nance.csv to ./workspace/Datasets/shop_orders_export_shop_id_5874055_from_2023-11-01_to_2023-11-30 - Phil Nance.csv\n",
            "Downloading file: endowment_gm.csv to ./workspace/Datasets/endowment_gm.csv\n",
            "Downloading file: Transaction List - Dawn Alderson.csv to ./workspace/Datasets/Transaction List - Dawn Alderson.csv\n",
            "Downloading file: high_portability_local_cellphone.csv to ./workspace/Datasets/high_portability_local_cellphone.csv\n",
            "Downloading file: Technical_services.csv to ./workspace/Datasets/Technical_services.csv\n",
            "Downloading file: MARKAS S.A. BOX Purchases.csv to ./workspace/Datasets/MARKAS S.A. BOX Purchases.csv\n",
            "Downloading file: 2023-12-21_transactions_export - Bailey Talley.csv to ./workspace/Datasets/2023-12-21_transactions_export - Bailey Talley.csv\n",
            "Downloading file: OCT-QA-Report-2023 - Marlene Portillo.csv to ./workspace/Datasets/OCT-QA-Report-2023 - Marlene Portillo.csv\n",
            "Downloading file: Bookshop Sales and Inventory Dataset.csv to ./workspace/Datasets/Bookshop Sales and Inventory Dataset.csv\n",
            "Downloading file: november_supermarket_sales_per_product.csv to ./workspace/Datasets/november_supermarket_sales_per_product.csv\n",
            "Downloading file: Art Gallery Spending Log 2017 - Erica Redling.csv to ./workspace/Datasets/Art Gallery Spending Log 2017 - Erica Redling.csv\n",
            "Downloading file: BranchTransac.csv to ./workspace/Datasets/BranchTransac.csv\n",
            "Downloading file: Sparkle and Sash Sales Detail 1_1_2022-6_30_2022 - Sheet1 (1) - Sparkle Sash.csv to ./workspace/Datasets/Sparkle and Sash Sales Detail 1_1_2022-6_30_2022 - Sheet1 (1) - Sparkle Sash.csv\n",
            "Downloading file: Cegin list of shifts of the month.csv to ./workspace/Datasets/Cegin list of shifts of the month.csv\n",
            "Downloading file: Purchase_RLC.csv to ./workspace/Datasets/Purchase_RLC.csv\n",
            "Downloading file: 2023 Inventory - Elisabeth Gracyalny.csv to ./workspace/Datasets/2023 Inventory - Elisabeth Gracyalny.csv\n",
            "Downloading file: production_costs_Bakery.csv to ./workspace/Datasets/production_costs_Bakery.csv\n",
            "Downloading file: metzeler_tires.csv to ./workspace/Datasets/metzeler_tires.csv\n",
            "Downloading file: E Plan Budget.csv to ./workspace/Datasets/E Plan Budget.csv\n",
            "Downloading file: Sales records  report_ 12_2021- 12_2023 (2 years) - Sheet1 - Chef Walt.csv to ./workspace/Datasets/Sales records  report_ 12_2021- 12_2023 (2 years) - Sheet1 - Chef Walt.csv\n",
            "Downloading file: Sale_Detail.csv to ./workspace/Datasets/Sale_Detail.csv\n",
            "Downloading file: Expenses_2015.csv to ./workspace/Datasets/Expenses_2015.csv\n",
            "Downloading file: online_retail.csv to ./workspace/Datasets/online_retail.csv\n",
            "Downloading file: Commercial_Inventory.csv to ./workspace/Datasets/Commercial_Inventory.csv\n",
            "Downloading file: sales_data JAN 2020 through DEC 2021 - DARKOTHICA - Mel.csv to ./workspace/Datasets/sales_data JAN 2020 through DEC 2021 - DARKOTHICA - Mel.csv\n",
            "Downloading file: Dairy 0051-01.csv to ./workspace/Datasets/Dairy 0051-01.csv\n",
            "Downloading file: Champro SKU Specs.csv to ./workspace/Datasets/Champro SKU Specs.csv\n",
            "Downloading file: Info_FTTH.xlsx - PPP_INFO.csv to ./workspace/Datasets/Info_FTTH.xlsx - PPP_INFO.csv\n",
            "Downloading file: Delivery_Sales Report.csv to ./workspace/Datasets/Delivery_Sales Report.csv\n",
            "Downloading file: Tecno_sales_2018-12-06_2023-12-06.csv to ./workspace/Datasets/Tecno_sales_2018-12-06_2023-12-06.csv\n",
            "Downloading file: Children's White Sales.csv to ./workspace/Datasets/Children's White Sales.csv\n",
            "Downloading file: data - Kayla Banger.csv to ./workspace/Datasets/data - Kayla Banger.csv\n",
            "Downloading file: Stamping_Dataset.csv to ./workspace/Datasets/Stamping_Dataset.csv\n",
            "Downloading file: LSC _ 2022_sales_activity_report - Casey Montante.csv to ./workspace/Datasets/LSC _ 2022_sales_activity_report - Casey Montante.csv\n",
            "Downloading file: BUILDING EXPENSES.csv to ./workspace/Datasets/BUILDING EXPENSES.csv\n",
            "Downloading file: BusinessReport-7-28-22 - Travis Colahan.csv to ./workspace/Datasets/BusinessReport-7-28-22 - Travis Colahan.csv\n",
            "Downloading file: Bybit-Derivatives-TradeHistory-20221001-20230111 - Syed Jafri.csv to ./workspace/Datasets/Bybit-Derivatives-TradeHistory-20221001-20230111 - Syed Jafri.csv\n",
            "Downloading file: Untitled spreadsheet - Melissa Jaycox.csv to ./workspace/Datasets/Untitled spreadsheet - Melissa Jaycox.csv\n",
            "Downloading file: VF Sample Dataset - rafael guzman.csv to ./workspace/Datasets/VF Sample Dataset - rafael guzman.csv\n",
            "Downloading file: SubSystemIO.csv to ./workspace/Datasets/SubSystemIO.csv\n",
            "Downloading file: businesspayrolls.csv to ./workspace/Datasets/businesspayrolls.csv\n",
            "Downloading file: C1_dbjg.csv to ./workspace/Datasets/C1_dbjg.csv\n",
            "Dataset download complete.\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "import os\n",
        "import sys\n",
        "import zipfile\n",
        "import shutil\n",
        "import re\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "# Version to download\n",
        "VERSION = \"0.1.0\"  # Version of the API\n",
        "\n",
        "# Define paths\n",
        "CONTENT_DIR = '/content'\n",
        "APIS_DIR = os.path.join(CONTENT_DIR, 'APIs')\n",
        "DBS_DIR = os.path.join(CONTENT_DIR, 'DBs')\n",
        "SCRIPTS_DIR = os.path.join(CONTENT_DIR, 'Scripts')\n",
        "FC_DIR = os.path.join(CONTENT_DIR, 'Schemas')\n",
        "ZIP_PATH = os.path.join(CONTENT_DIR, f'APIs_V{VERSION}.zip')\n",
        "\n",
        "# Google Drive Folder ID where versioned APIs zip files are stored\n",
        "APIS_FOLDER_ID = '1QpkAZxXhVFzIbm8qPGPRP1YqXEvJ4uD4'\n",
        "\n",
        "# List of items to extract from the zip file\n",
        "ITEMS_TO_EXTRACT = ['APIs/', 'DBs/', 'Scripts/']\n",
        "\n",
        "# Clean up existing directories and files\n",
        "for path in [APIS_DIR, DBS_DIR, SCRIPTS_DIR, FC_DIR, ZIP_PATH]:\n",
        "    if os.path.exists(path):\n",
        "        if os.path.isdir(path):\n",
        "            shutil.rmtree(path)\n",
        "        else:\n",
        "            os.remove(path)\n",
        "\n",
        "# Authenticate and create the drive service\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "# Helper function to download a file from Google Drive\n",
        "def download_drive_file(service, file_id, output_path, file_name=None, show_progress=True):\n",
        "    \"\"\"Downloads a file from Google Drive\"\"\"\n",
        "    destination = output_path\n",
        "    request = service.files().get_media(fileId=file_id)\n",
        "    with io.FileIO(destination, 'wb') as fh:\n",
        "        downloader = MediaIoBaseDownload(fh, request)\n",
        "        done = False\n",
        "        while not done:\n",
        "            status, done = downloader.next_chunk()\n",
        "            if show_progress:\n",
        "                print(f\"Download progress: {int(status.progress() * 100)}%\")\n",
        "\n",
        "\n",
        "# 1. List files in the specified APIs folder\n",
        "print(f\"Searching for APIs zip file with version {VERSION} in folder: {APIS_FOLDER_ID}...\")\n",
        "apis_file_id = None\n",
        "\n",
        "try:\n",
        "    query = f\"'{APIS_FOLDER_ID}' in parents and trashed=false\"\n",
        "    results = drive_service.files().list(q=query, fields=\"files(id, name)\").execute()\n",
        "    files = results.get('files', [])\n",
        "    for file in files:\n",
        "        file_name = file.get('name', '')\n",
        "        if file_name.lower() == f'apis_v{VERSION.lower()}.zip':\n",
        "            apis_file_id = file.get('id')\n",
        "            print(f\"Found matching file: {file_name} (ID: {apis_file_id})\")\n",
        "            break\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while listing files in Google Drive: {e}\")\n",
        "\n",
        "if not apis_file_id:\n",
        "    print(f\"Error: Could not find APIs zip file with version {VERSION} in the specified folder.\")\n",
        "    sys.exit(\"Required APIs zip file not found.\")\n",
        "\n",
        "# 2. Download the found APIs zip file\n",
        "print(f\"Downloading APIs zip file with ID: {apis_file_id}...\")\n",
        "download_drive_file(drive_service, apis_file_id, ZIP_PATH, file_name=f'APIs_V{VERSION}.zip')\n",
        "\n",
        "# 3. Extract specific items from the zip file to /content\n",
        "print(f\"Extracting specific items from {ZIP_PATH} to {CONTENT_DIR}...\")\n",
        "try:\n",
        "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
        "        zip_contents = zip_ref.namelist()\n",
        "\n",
        "        for member in zip_contents:\n",
        "            extracted = False\n",
        "            for item_prefix in ITEMS_TO_EXTRACT:\n",
        "              if member == item_prefix or member.startswith(item_prefix):\n",
        "                    zip_ref.extract(member, CONTENT_DIR)\n",
        "                    extracted = True\n",
        "                    break\n",
        "\n",
        "except zipfile.BadZipFile:\n",
        "    print(f\"Error: The downloaded file at {ZIP_PATH} is not a valid zip file.\")\n",
        "    sys.exit(\"Invalid zip file downloaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during extraction: {e}\")\n",
        "    sys.exit(\"Extraction failed.\")\n",
        "\n",
        "\n",
        "# 4. Clean up\n",
        "if os.path.exists(ZIP_PATH):\n",
        "    os.remove(ZIP_PATH)\n",
        "\n",
        "# 5. Add APIs to path\n",
        "if os.path.exists(APIS_DIR):\n",
        "    sys.path.append(APIS_DIR)\n",
        "else:\n",
        "    print(f\"Error: APIS directory not found at {APIS_DIR} after extraction. Cannot add to path.\")\n",
        "\n",
        "# 6. Quick verification\n",
        "# Check for the presence of the extracted items\n",
        "verification_paths = [APIS_DIR, DBS_DIR, SCRIPTS_DIR]\n",
        "all_present = True\n",
        "print(\"\\nVerifying extracted items:\")\n",
        "for path in verification_paths:\n",
        "    if os.path.exists(path):\n",
        "        print(f\"✅ {path} is present.\")\n",
        "    else:\n",
        "        print(f\"❌ {path} is MISSING!\")\n",
        "        all_present = False\n",
        "\n",
        "if all_present:\n",
        "    print(f\"\\n✅ Setup complete! Required items extracted to {CONTENT_DIR}.\")\n",
        "else:\n",
        "    print(\"\\n❌ Setup failed! Not all required items were extracted.\")\n",
        "\n",
        "# 7. Generate Schemas\n",
        "from Scripts.FCSpec import generate_package_schema\n",
        "\n",
        "print(\"\\nGenerating FC Schemas\")\n",
        "os.makedirs(FC_DIR, exist_ok=True)\n",
        "\n",
        "# Change working directory to the source folder\n",
        "os.chdir(APIS_DIR)\n",
        "\n",
        "# Iterate through the packages in the /content/APIs directory\n",
        "for package_name in os.listdir(APIS_DIR):\n",
        "    package_path = os.path.join(APIS_DIR, package_name)\n",
        "\n",
        "    # Check if it's a directory (to avoid processing files)\n",
        "    if os.path.isdir(package_path):\n",
        "        # Call the function to generate schema for the current package\n",
        "        generate_package_schema(package_path, output_folder_path=FC_DIR)\n",
        "print(f\"✅ Successfully generated {len(os.listdir(FC_DIR))} FC Schemas to {FC_DIR}\")\n",
        "os.chdir(CONTENT_DIR)\n",
        "\n",
        "\n",
        "def download_drive_folder(service, folder_id, destination_path):\n",
        "    \"\"\"\n",
        "    Recursively downloads all files in a Google Drive folder using the `download_drive_file`\n",
        "    \"\"\"\n",
        "    os.makedirs(destination_path, exist_ok=True)\n",
        "\n",
        "    query = f\"'{folder_id}' in parents and trashed=false\"\n",
        "    page_token = None\n",
        "\n",
        "    while True:\n",
        "        results = service.files().list(\n",
        "            q=query,\n",
        "            spaces='drive',\n",
        "            fields='nextPageToken, files(id, name, mimeType)',\n",
        "            pageToken=page_token\n",
        "        ).execute()\n",
        "\n",
        "        for item in results.get('files', []):\n",
        "            file_id = item['id']\n",
        "            file_name = item['name']\n",
        "            mime_type = item['mimeType']\n",
        "\n",
        "            if mime_type == 'application/vnd.google-apps.folder':\n",
        "                # Recursively download subfolders\n",
        "                new_path = os.path.join(destination_path, file_name)\n",
        "                print(f\"Creating subfolder and downloading: {new_path}\")\n",
        "                download_drive_folder(service, file_id, new_path)\n",
        "            else:\n",
        "                # Construct full file path and pass it as output_path\n",
        "                full_path = os.path.join(destination_path, file_name)\n",
        "                print(f\"Downloading file: {file_name} to {full_path}\")\n",
        "                download_drive_file(service, file_id, full_path, file_name=file_name, show_progress=False)\n",
        "\n",
        "        page_token = results.get('nextPageToken', None)\n",
        "        if not page_token:\n",
        "            break\n",
        "\n",
        "# --- Configuration for Dataset Download ---\n",
        "FOLDER_ID = \"1tZqZB1vAxp4TTxbPm6O2YjfkZD4FM-ml\"\n",
        "DATASET_FOLDER = \"./workspace/Datasets\"\n",
        "\n",
        "print(f\"Starting download of folder {FOLDER_ID} to {DATASET_FOLDER}...\")\n",
        "download_drive_folder(drive_service, FOLDER_ID, DATASET_FOLDER)\n",
        "print(\"Dataset download complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f9648a0",
      "metadata": {
        "id": "4f9648a0"
      },
      "source": [
        "## Install Dependencies and Clone Repositories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b6a2b91",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1b6a2b91",
        "outputId": "abdbd053-3626-4cc0-81b4-f91765ed9952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytest==8.3.5 in /usr/local/lib/python3.11/dist-packages (from -r /content/APIs/requirements.txt (line 1)) (8.3.5)\n",
            "Collecting coverage==7.8.0 (from -r /content/APIs/requirements.txt (line 2))\n",
            "  Downloading coverage-7.8.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from -r /content/APIs/requirements.txt (line 3)) (2.9.0.post0)\n",
            "Collecting pydantic==2.11.4 (from pydantic[email]==2.11.4->-r /content/APIs/requirements.txt (line 4))\n",
            "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator==2.2.0 (from -r /content/APIs/requirements.txt (line 5))\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting thefuzz==0.22.1 (from -r /content/APIs/requirements.txt (line 6))\n",
            "  Downloading thefuzz-0.22.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting python-Levenshtein==0.27.1 (from -r /content/APIs/requirements.txt (line 7))\n",
            "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting google-genai==1.14.0 (from -r /content/APIs/requirements.txt (line 8))\n",
            "  Downloading google_genai-1.14.0-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting python-dotenv==1.1.0 (from -r /content/APIs/requirements.txt (line 9))\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: sentence-transformers==4.1.0 in /usr/local/lib/python3.11/dist-packages (from -r /content/APIs/requirements.txt (line 10)) (4.1.0)\n",
            "Collecting chromadb==1.0.8 (from -r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading chromadb-1.0.8-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting json-repair==0.44.1 (from -r /content/APIs/requirements.txt (line 12))\n",
            "  Downloading json_repair-0.44.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: beautifulsoup4==4.13.4 in /usr/local/lib/python3.11/dist-packages (from -r /content/APIs/requirements.txt (line 13)) (4.13.4)\n",
            "Collecting readability-lxml==0.8.4.1 (from -r /content/APIs/requirements.txt (line 14))\n",
            "  Downloading readability_lxml-0.8.4.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.11/dist-packages (from -r /content/APIs/requirements.txt (line 15)) (2.32.3)\n",
            "Collecting pymongo==4.13.0 (from -r /content/APIs/requirements.txt (line 16))\n",
            "  Downloading pymongo-4.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting mongomock==4.3.0 (from -r /content/APIs/requirements.txt (line 17))\n",
            "  Downloading mongomock-4.3.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pymongo-schema==0.4.1 (from -r /content/APIs/requirements.txt (line 18))\n",
            "  Downloading pymongo_schema-0.4.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting duckdb==1.3.0 (from -r /content/APIs/requirements.txt (line 19))\n",
            "  Downloading duckdb-1.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting sqlglot==26.25.3 (from sqlglot[rs]==26.25.3->-r /content/APIs/requirements.txt (line 20))\n",
            "  Downloading sqlglot-26.25.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting playwright==1.52.0 (from -r /content/APIs/requirements.txt (line 21))\n",
            "  Downloading playwright-1.52.0-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: nest-asyncio==1.6.0 in /usr/local/lib/python3.11/dist-packages (from -r /content/APIs/requirements.txt (line 22)) (1.6.0)\n",
            "Requirement already satisfied: docstring_parser==0.16 in /usr/local/lib/python3.11/dist-packages (from -r /content/APIs/requirements.txt (line 23)) (0.16)\n",
            "Collecting quickjs==1.19.4 (from -r /content/APIs/requirements.txt (line 24))\n",
            "  Downloading quickjs-1.19.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (590 bytes)\n",
            "Collecting mermaid-py==0.8.0 (from -r /content/APIs/requirements.txt (line 25))\n",
            "  Downloading mermaid_py-0.8.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: tzlocal==5.3.1 in /usr/local/lib/python3.11/dist-packages (from -r /content/APIs/requirements.txt (line 26)) (5.3.1)\n",
            "Collecting qdrant-client==1.14.3 (from -r /content/APIs/requirements.txt (line 27))\n",
            "  Downloading qdrant_client-1.14.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting Whoosh==2.7.4 (from -r /content/APIs/requirements.txt (line 28))\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: gdown==5.2.0 in /usr/local/lib/python3.11/dist-packages (from -r /content/APIs/requirements.txt (line 30)) (5.2.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest==8.3.5->-r /content/APIs/requirements.txt (line 1)) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytest==8.3.5->-r /content/APIs/requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest==8.3.5->-r /content/APIs/requirements.txt (line 1)) (1.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil==2.9.0.post0->-r /content/APIs/requirements.txt (line 3)) (1.17.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic==2.11.4->pydantic[email]==2.11.4->-r /content/APIs/requirements.txt (line 4)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic==2.11.4->pydantic[email]==2.11.4->-r /content/APIs/requirements.txt (line 4)) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic==2.11.4->pydantic[email]==2.11.4->-r /content/APIs/requirements.txt (line 4)) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic==2.11.4->pydantic[email]==2.11.4->-r /content/APIs/requirements.txt (line 4)) (0.4.1)\n",
            "Collecting dnspython>=2.0.0 (from email_validator==2.2.0->-r /content/APIs/requirements.txt (line 5))\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email_validator==2.2.0->-r /content/APIs/requirements.txt (line 5)) (3.10)\n",
            "Collecting rapidfuzz<4.0.0,>=3.0.0 (from thefuzz==0.22.1->-r /content/APIs/requirements.txt (line 6))\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting Levenshtein==0.27.1 (from python-Levenshtein==0.27.1->-r /content/APIs/requirements.txt (line 7))\n",
            "  Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from google-genai==1.14.0->-r /content/APIs/requirements.txt (line 8)) (4.9.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-genai==1.14.0->-r /content/APIs/requirements.txt (line 8)) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai==1.14.0->-r /content/APIs/requirements.txt (line 8)) (0.28.1)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai==1.14.0->-r /content/APIs/requirements.txt (line 8)) (15.0.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10)) (4.53.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10)) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10)) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10)) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10)) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10)) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10)) (11.2.1)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (1.2.2.post1)\n",
            "Collecting fastapi==0.115.9 (from chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (2.0.2)\n",
            "Collecting posthog>=2.4.0 (from chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading posthog-6.0.4-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.55b1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (0.21.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (1.73.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (0.16.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (3.10.18)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (4.24.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4==4.13.4->-r /content/APIs/requirements.txt (line 13)) (2.7)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from readability-lxml==0.8.4.1->-r /content/APIs/requirements.txt (line 14)) (5.2.0)\n",
            "Requirement already satisfied: lxml[html_clean] in /usr/local/lib/python3.11/dist-packages (from readability-lxml==0.8.4.1->-r /content/APIs/requirements.txt (line 14)) (5.4.0)\n",
            "Collecting cssselect (from readability-lxml==0.8.4.1->-r /content/APIs/requirements.txt (line 14))\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3->-r /content/APIs/requirements.txt (line 15)) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3->-r /content/APIs/requirements.txt (line 15)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3->-r /content/APIs/requirements.txt (line 15)) (2025.6.15)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from mongomock==4.3.0->-r /content/APIs/requirements.txt (line 17)) (2025.2)\n",
            "Collecting sentinels (from mongomock==4.3.0->-r /content/APIs/requirements.txt (line 17))\n",
            "  Downloading sentinels-1.0.0.tar.gz (4.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docopt (from pymongo-schema==0.4.1->-r /content/APIs/requirements.txt (line 18))\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ete3 (from pymongo-schema==0.4.1->-r /content/APIs/requirements.txt (line 18))\n",
            "  Downloading ete3-3.1.3.tar.gz (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from pymongo-schema==0.4.1->-r /content/APIs/requirements.txt (line 18)) (2.2.2)\n",
            "Collecting xlwt (from pymongo-schema==0.4.1->-r /content/APIs/requirements.txt (line 18))\n",
            "  Downloading xlwt-1.3.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting xlsxwriter (from pymongo-schema==0.4.1->-r /content/APIs/requirements.txt (line 18))\n",
            "  Downloading xlsxwriter-3.2.5-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (from pymongo-schema==0.4.1->-r /content/APIs/requirements.txt (line 18)) (3.1.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from pymongo-schema==0.4.1->-r /content/APIs/requirements.txt (line 18)) (3.1.6)\n",
            "Requirement already satisfied: future>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from pymongo-schema==0.4.1->-r /content/APIs/requirements.txt (line 18)) (1.0.0)\n",
            "Collecting pyee<14,>=13 (from playwright==1.52.0->-r /content/APIs/requirements.txt (line 21))\n",
            "  Downloading pyee-13.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright==1.52.0->-r /content/APIs/requirements.txt (line 21)) (3.2.3)\n",
            "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client==1.14.3->-r /content/APIs/requirements.txt (line 27))\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client==1.14.3->-r /content/APIs/requirements.txt (line 27)) (5.29.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown==5.2.0->-r /content/APIs/requirements.txt (line 30)) (3.18.0)\n",
            "Collecting sqlglotrs==0.6.1 (from sqlglot[rs]==26.25.3->-r /content/APIs/requirements.txt (line 20))\n",
            "  Downloading sqlglotrs-0.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (545 bytes)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai==1.14.0->-r /content/APIs/requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (1.2.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai==1.14.0->-r /content/APIs/requirements.txt (line 8)) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai==1.14.0->-r /content/APIs/requirements.txt (line 8)) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai==1.14.0->-r /content/APIs/requirements.txt (line 8)) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai==1.14.0->-r /content/APIs/requirements.txt (line 8)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai==1.14.0->-r /content/APIs/requirements.txt (line 8)) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client==1.14.3->-r /content/APIs/requirements.txt (line 27)) (4.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10)) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10)) (1.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (0.26.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (3.3.1)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.55b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.55b1-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting opentelemetry-instrumentation==0.55b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading opentelemetry_instrumentation-0.55b1-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-util-http==0.55b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading opentelemetry_util_http-0.55b1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (1.17.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading asgiref-3.9.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (1.9.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (2.19.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10)) (3.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10)) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10)) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10)) (0.5.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->pymongo-schema==0.4.1->-r /content/APIs/requirements.txt (line 18)) (3.0.2)\n",
            "Collecting lxml_html_clean (from lxml[html_clean]->readability-lxml==0.8.4.1->-r /content/APIs/requirements.txt (line 14))\n",
            "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl->pymongo-schema==0.4.1->-r /content/APIs/requirements.txt (line 18)) (2.0.0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->pymongo-schema==0.4.1->-r /content/APIs/requirements.txt (line 18)) (2025.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==5.2.0->-r /content/APIs/requirements.txt (line 30)) (1.7.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10)) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers==4.1.0->-r /content/APIs/requirements.txt (line 10)) (3.6.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client==1.14.3->-r /content/APIs/requirements.txt (line 27)) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client==1.14.3->-r /content/APIs/requirements.txt (line 27)) (4.1.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11)) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai==1.14.0->-r /content/APIs/requirements.txt (line 8)) (0.6.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==1.0.8->-r /content/APIs/requirements.txt (line 11))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Downloading coverage-7.8.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.0/244.0 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.9/443.9 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading thefuzz-0.22.1-py3-none-any.whl (8.2 kB)\n",
            "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading google_genai-1.14.0-py3-none-any.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.9/168.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading chromadb-1.0.8-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/18.9 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json_repair-0.44.1-py3-none-any.whl (22 kB)\n",
            "Downloading readability_lxml-0.8.4.1-py3-none-any.whl (19 kB)\n",
            "Downloading pymongo-4.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mongomock-4.3.0-py2.py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymongo_schema-0.4.1-py3-none-any.whl (29 kB)\n",
            "Downloading duckdb-1.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sqlglot-26.25.3-py3-none-any.whl (465 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.4/465.4 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading playwright-1.52.0-py3-none-manylinux1_x86_64.whl (45.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading quickjs-1.19.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mermaid_py-0.8.0-py3-none-any.whl (31 kB)\n",
            "Downloading qdrant_client-1.14.3-py3-none-any.whl (328 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.0/329.0 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sqlglotrs-0.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (340 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.5/340.5 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.34.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.55b1-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.55b1-py3-none-any.whl (31 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.55b1-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.55b1-py3-none-any.whl (7.3 kB)\n",
            "Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading posthog-6.0.4-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyee-13.0.0-py3-none-any.whl (15 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading xlsxwriter-3.2.5-py3-none-any.whl (172 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xlwt-1.3.0-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.0/100.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
            "Downloading asgiref-3.9.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika, docopt, ete3, sentinels\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=b0f6465f72b9879e3fb0a9e4f1e3b7de1627aa6039202d1ced6bcb641a1291c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=09303f536f3d5c22a022eab6adb890b0869baf827cccbb241289d9e24a6a054d\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "  Building wheel for ete3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ete3: filename=ete3-3.1.3-py3-none-any.whl size=2273786 sha256=587e369d103e4a43ba8d7c33102a9fd00ab3169101aab416270d759adfec6050\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/a8/60/0a29caa9f8ceb7316704be63c1578ab13c36668abb646366ac\n",
            "  Building wheel for sentinels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentinels: filename=sentinels-1.0.0-py3-none-any.whl size=3172 sha256=7a4082f119f60f6bae6d92bfc59c2bcbc9e14560df26fb5cc9b504b7b88b2e81\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e6/05/d0ca91a2c6be3e4b2a6b4e721fe778f9186b9a383ea05300e8\n",
            "Successfully built pypika docopt ete3 sentinels\n",
            "Installing collected packages: xlwt, Whoosh, sentinels, quickjs, pypika, ete3, durationpy, docopt, xlsxwriter, uvloop, sqlglotrs, sqlglot, rapidfuzz, python-dotenv, pyee, portalocker, overrides, opentelemetry-util-http, opentelemetry-proto, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mongomock, mmh3, lxml_html_clean, json-repair, humanfriendly, httptools, duckdb, dnspython, cssselect, coverage, bcrypt, backoff, asgiref, watchfiles, thefuzz, starlette, pymongo, pydantic, posthog, playwright, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cudnn-cu12, mermaid-py, Levenshtein, email_validator, coloredlogs, readability-lxml, python-Levenshtein, pymongo-schema, opentelemetry-semantic-conventions, onnxruntime, nvidia-cusolver-cu12, kubernetes, google-genai, fastapi, qdrant-client, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: sqlglot\n",
            "    Found existing installation: sqlglot 25.20.2\n",
            "    Uninstalling sqlglot-25.20.2:\n",
            "      Successfully uninstalled sqlglot-25.20.2\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: duckdb\n",
            "    Found existing installation: duckdb 1.2.2\n",
            "    Uninstalling duckdb-1.2.2:\n",
            "      Successfully uninstalled duckdb-1.2.2\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.46.2\n",
            "    Uninstalling starlette-0.46.2:\n",
            "      Successfully uninstalled starlette-0.46.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.7\n",
            "    Uninstalling pydantic-2.11.7:\n",
            "      Successfully uninstalled pydantic-2.11.7\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: google-genai\n",
            "    Found existing installation: google-genai 1.24.0\n",
            "    Uninstalling google-genai-1.24.0:\n",
            "      Successfully uninstalled google-genai-1.24.0\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.115.14\n",
            "    Uninstalling fastapi-0.115.14:\n",
            "      Successfully uninstalled fastapi-0.115.14\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ibis-framework 9.5.0 requires sqlglot<25.21,>=23.4, but you have sqlglot 26.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Levenshtein-0.27.1 Whoosh-2.7.4 asgiref-3.9.1 backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.8 coloredlogs-15.0.1 coverage-7.8.0 cssselect-1.3.0 dnspython-2.7.0 docopt-0.6.2 duckdb-1.3.0 durationpy-0.10 email_validator-2.2.0 ete3-3.1.3 fastapi-0.115.9 google-genai-1.14.0 httptools-0.6.4 humanfriendly-10.0 json-repair-0.44.1 kubernetes-33.1.0 lxml_html_clean-0.4.2 mermaid-py-0.8.0 mmh3-5.1.0 mongomock-4.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnxruntime-1.22.0 opentelemetry-api-1.34.1 opentelemetry-exporter-otlp-proto-common-1.34.1 opentelemetry-exporter-otlp-proto-grpc-1.34.1 opentelemetry-instrumentation-0.55b1 opentelemetry-instrumentation-asgi-0.55b1 opentelemetry-instrumentation-fastapi-0.55b1 opentelemetry-proto-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 opentelemetry-util-http-0.55b1 overrides-7.7.0 playwright-1.52.0 portalocker-2.10.1 posthog-6.0.4 pydantic-2.11.4 pyee-13.0.0 pymongo-4.13.0 pymongo-schema-0.4.1 pypika-0.48.9 python-Levenshtein-0.27.1 python-dotenv-1.1.0 qdrant-client-1.14.3 quickjs-1.19.4 rapidfuzz-3.13.0 readability-lxml-0.8.4.1 sentinels-1.0.0 sqlglot-26.25.3 sqlglotrs-0.6.1 starlette-0.45.3 thefuzz-0.22.1 uvloop-0.21.0 watchfiles-1.1.0 xlsxwriter-3.2.5 xlwt-1.3.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "926be67e18d140e2ad92e66c5aa2d6e6",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install -r /content/APIs/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57511683",
      "metadata": {
        "id": "57511683"
      },
      "source": [
        "  ## Import APIs and initiate DBs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "355b2478",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "355b2478",
        "outputId": "02d58d61-009f-4df1-c552-564494b655bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hydrating database from directory: ./workspace\n",
            "Database hydration complete.\n"
          ]
        }
      ],
      "source": [
        "import terminal\n",
        "import os\n",
        "from terminal.SimulationEngine.utils import hydrate_db_from_directory\n",
        "from terminal.SimulationEngine.db import DB\n",
        "\n",
        "# Set environment variables for this session\n",
        "os.environ['GOOGLE_API_KEY'] = \"AIzaSyCkQFuIGGpONvrg1FEF8_mvdWzw9TYClr8\"\n",
        "os.environ['GEMINI_API_KEY_2'] = \"AIzaSyB1Bv5-bJrA6SH65zFpN0eL3sgFHKe7fFs\"\n",
        "os.environ['DEFAULT_GEMINI_MODEL_NAME'] = \"gemini-2.5-pro-preview-03-25\"\n",
        "\n",
        "# --- Configuration ---\n",
        "# For non-GitHub projects, we assume files are in a specific dataset folder.\n",
        "DATASET_FOLDER = \"./workspace\"\n",
        "\n",
        "# --- Load States ---\n",
        "# For terminal, we hydrate the database from a directory.\n",
        "# This makes the files within DATASET_FOLDER available to the simulation.\n",
        "print(f\"Hydrating database from directory: {DATASET_FOLDER}\")\n",
        "hydrate_db_from_directory(DB, DATASET_FOLDER)\n",
        "print(\"Database hydration complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99375428",
      "metadata": {
        "id": "99375428"
      },
      "source": [
        "# Initial Assertion\n",
        "\n",
        "1. Assert that the dataset `2_Debits_and_Credits_Purchases.csv` exists in datasets directory.\n",
        "2. Assert that above has `md5Checksum \"b19ce41e7c74cd9de6fc95a882803ab4\"`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "132623a0",
      "metadata": {
        "id": "132623a0"
      },
      "outputs": [],
      "source": [
        "import terminal\n",
        "from terminal.SimulationEngine.custom_errors import CommandExecutionError\n",
        "from Scripts.assertions_utils import *\n",
        "\n",
        "# Define constants based on the scenario description\n",
        "file_name = \"2_Debits_and_Credits_Purchases.csv\"\n",
        "# Corrected directory path based on the error message and additional_data\n",
        "datasets_directory_path = \"./Datasets\"\n",
        "full_file_path = f\"{datasets_directory_path}/{file_name}\"\n",
        "expected_md5_checksum = \"b19ce41e7c74cd9de6fc95a882803ab4\"\n",
        "\n",
        "# Assertion 1: Assert that the dataset exists in the datasets directory.\n",
        "# List the contents of the datasets directory.\n",
        "file_exists = False\n",
        "try:\n",
        "  ls_command = f\"ls \\\"{datasets_directory_path}\\\"\" # Quoted to handle potential spaces in directory names, though not strictly needed here\n",
        "  ls_result = terminal.run_command(ls_command)\n",
        "  ls_stdout = ls_result.get('stdout', '')\n",
        "\n",
        "  # Check if the file_name is in the list of files.\n",
        "  # .splitlines() is used to handle filenames correctly, assuming `ls` outputs one entry per line.\n",
        "  file_exists = compare_is_list_subset(file_name,ls_stdout.splitlines())\n",
        "except CommandExecutionError:\n",
        "    # This occurs if `ls` command fails (e.g., file not found).\n",
        "    # `file_exists` remains False.\n",
        "    pass\n",
        "\n",
        "assert file_exists, f\"File '{file_name}' not found in directory '{datasets_directory_path}'.\"\n",
        "\n",
        "\n",
        "# Assertion 2: Assert that the file has the specified md5Checksum.\n",
        "# Construct the md5sum command with the full file path quoted to handle spaces.\n",
        "actual_md5_checksum = \"\"\n",
        "try:\n",
        "  md5sum_command = f'md5sum \"{full_file_path}\"'\n",
        "  md5sum_result = terminal.run_command(md5sum_command)\n",
        "  md5sum_stdout = md5sum_result.get('stdout', '')\n",
        "\n",
        "  # Parse the md5sum output. Expected format: \"checksum  filename\"\n",
        "  # .strip() removes leading/trailing whitespace, .split() splits by space.\n",
        "  # The first part is the checksum.\n",
        "  actual_md5_checksum_parts = md5sum_stdout.strip().split()\n",
        "  if actual_md5_checksum_parts: # Check if the list is not empty\n",
        "      actual_md5_checksum = actual_md5_checksum_parts[0]\n",
        "except CommandExecutionError:\n",
        "    # This occurs if `md5sum` command fails.\n",
        "    # `actual_md5_checksum` remains its initial value (empty string),\n",
        "    # causing the assertion to fail as intended if the checksum cannot be verified.\n",
        "    pass\n",
        "\n",
        "assert compare_strings(actual_md5_checksum, expected_md5_checksum), f\"MD5 checksum mismatch for file '{file_name}'. Expected: '{expected_md5_checksum}', Got: '{actual_md5_checksum}'.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abc09458",
      "metadata": {
        "id": "abc09458"
      },
      "source": [
        "# Action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edbb273a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edbb273a",
        "outputId": "88158fb0-7bcb-4727-bc08-71326be9475c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'message': 'Command completed with exit code 0. Workspace state updated.',\n",
              " 'stdout': \"2020 sales - Shannon O.csv\\n2022 Districts Monthly Transfer Dataset.csv\\n2023-12-21_transactions_export - Bailey Talley.csv\\n2023 Inventory - Elisabeth Gracyalny.csv\\n2201_VoucherCheker.csv\\n2_Debits_and_Credits_Purchases.csv\\nABM_WAGES.csv\\nAccounting - Jake Chase.csv\\nAccumulatorReadingsReport.csv\\nairbnb_tax_01_2023-01_2024 - Philip Ferraro.csv\\nArt Gallery Spending Log 2017 - Erica Redling.csv\\nbalance_sheet.csv\\nBank_accreditations.csv\\nBill_A2.csv\\nBlueprint_budget.csv\\nBold21 Data set - orders_export_1.csv\\nBookshop Sales and Inventory Dataset.csv\\nBranchTransac.csv\\nBUILDING EXPENSES.csv\\nburbujas_sales_july.csv\\nBurguerhouseJuly21.csv\\nBurguerhouseJuly21csv.csv\\nbusinesspayrolls.csv\\nBusinessReport-7-28-22 - Travis Colahan.csv\\nbusiness users.csv\\nBybit-Derivatives-TradeHistory-20221001-20230111 - Syed Jafri.csv\\nC1_dbjg.csv\\nCampaigns BIMO.csv\\nCars_Sales_22-23.csv\\ncash_flow_Bakery.csv\\nCegin list of shifts of the month.csv\\nChampro SKU Specs.csv\\nChecks.csv\\nChemicalProductsApplications.csv\\nchicken_groceries_prices.csv\\nChildren's White Sales.csv\\nCLIENTS AB USD_CLEAN.csv\\nClockify Detailed Time Report - 2023.csv\\nCOMAFI AND SUPER HISTORY.csv\\nCommercial_Inventory.csv\\nComparison with Wal Mart Catalog Stock.csv\\nConstruction_costs_delivery.csv\\nContent Delivery Data Set - Jacob Goldberg.csv\\nCristine's Corner January - Cristine Marquez.csv\\nDairy 0051-01.csv\\nDairy Solids (fat+protein).csv\\ndata-export (3) - Harsh Bakshi.csv\\ndata - Kayla Banger.csv\\nDataset for Solar Panel Cleaning - Nestor Sanchez.csv\\nData Set Remotasks - Anita Portnoy.csv\\nDelivery_Sales Report.csv\\nEarnings And Placements - statement.csv - Jonathan Foster.csv\\neBay-ListingsSalesReport-Dec-22-2023-08_21_39-0700-12145337749 - Karina Arango.csv\\nendowment_gm.csv\\nE Plan Budget.csv\\nEtsySoldOrders2021_2023 - Sara Gerges.csv\\nExpenses_20152.csv\\nExpenses_2015.csv\\nExpense_summary.csv\\n[External] order_items.csv\\nfinancial_credits.csv\\nFirst School Term 2023_2024 - Robert Russell.csv\\nFragrance June.csv\\nGARDEN LOG - Visits (1) - Erica Redling.csv\\nGwynstone.Originz.dat - Gwynne Rife.csv\\nHEALTHCARE AFFILIATE MANAGEMENT.csv\\nHigher Ed Threads 2023 - Phil Nance.csv\\nhigh_portability_local_cellphone.csv\\nICEQueries_Files_md5Checksum.csv\\nIllinoisChicago_SuiteRentals_Aug2023 - Dalron J. Robertson.csv\\nInfo_FTTH.xlsx - PPP_INFO.csv\\nJAZ Ceramics.csv\\nJuana Events Rentals.csv\\nKiltX Web3 Summits Anonymized Data Keri Kilty - keri kilty.csv\\nKookyArtsAi DATABASE - Christy Rivers.csv\\nLog_2016_2020_Redacted - Adam Johnson.csv\\nLSC _ 2022_sales_activity_report - Casey Montante.csv\\nMama Milk Flow Data - paul miles.csv\\nMarAug2023_ProjInst_Dataset - Diana.csv\\nMarch Sales.csv\\nMARKAS S.A. BOX Purchases.csv\\nMaterials_CTM.csv\\nmetzeler_tires.csv\\nmill_operations.csv\\nMonthly Cash Flow.csv\\nnovember_supermarket_sales_per_product.csv\\nOCT-QA-Report-2023 - Marlene Portillo.csv\\nonline_retail.csv\\nOOC-55 - William Webster (SickBoy).csv\\norder.list.export 2023-12-22 (2) - MOZZAM SHAHID.csv\\norders - orders - Kimm Topping.csv\\nPaper_Pen_Pavilion_Sales_09_12 - Cordejah Walker.csv\\nParcels_2023Q1_CO - Guillermo Pardon.csv\\nPayroll_oct_16-31.csv\\npayroll_sheet.csv\\nPrices July 2022 EQUILIBRIO.csv\\nprices_ss.csv\\nprodqualityanalysis.csv\\nProduct Dims.csv\\nproduction_costs_Bakery.csv\\nProductivity2.csv\\npublic_lighting.csv\\nPurchase_invoice_graphics_company.csv\\nPurchase_orders_GAP_ZEN.csv\\nPurchase_RLC.csv\\nQuotewk.csv\\nReport_periodical_records.csv\\nRetrogasm sales 2023 1 - Rachel Robinson.csv\\nR Plan Budget.csv\\nrtshare - Kris Best.csv\\nSalary and Equity Data.csv\\nSale_Detail.csv\\nsales.csv\\nsales_data JAN 2020 through DEC 2021 - DARKOTHICA - Mel.csv\\nSalesFiguresQ1Q2 - Travis Colahan.csv\\nSales records  report_ 12_2021- 12_2023 (2 years) - Sheet1 - Chef Walt.csv\\nSales_Report.csv\\nSales_RLC2.csv\\nServerProcesses.csv\\nSheet1 - Brooks Chiro Rehab.csv\\nshop_orders_export_shop_id_5874055_from_2023-11-01_to_2023-11-30 - Phil Nance.csv\\nSparkle and Sash Sales Detail 1_1_2022-6_30_2022 - Sheet1 (1) - Sparkle Sash.csv\\nStamping_Dataset.csv\\nStorage (version 1).csv - Sheet1 - Erica Redling.csv\\nStore Survey - Patrick Bernier.csv\\nStudent supplies delivery costs Fall 2023 - Robert Russell.csv\\nSubmission id - Miss Kaye Estacio.csv\\nSubSystemIO.csv\\nsupplies.csv\\nSurvey Meters.csv\\nSurvey - Videogames - Brazil 2015 - Alexandre Papanis.csv\\nTD_HET Shipping Report - Phil Nance.csv\\nTechnical_services.csv\\nTecno_MP_activities-feeder_collection-20231206183310-efa71.csv\\nTecno_sales_2018-12-06_2023-12-06.csv\\nToggl Detailed Time Report - 2022.csv\\ntorque and flow performance.csv\\nTransaction List - Dawn Alderson.csv\\nTruck Repairs for 2023 - Sheet1 - Dr. Victoria Gamble.csv\\nU0089_Clients.csv\\nUntitled spreadsheet - Melissa Jaycox.csv\\nvaccinations_city_flores.csv\\nValues Survey - June 2023.csv\\nVat_purchases_journal_02-2023.csv\\nVat_sales_ledger 02-2023.csv\\nVector360-TravelResponse - Ernesto Herrero.csv\\nVF Sample Dataset - rafael guzman.csv\\nvisits_2023-11-01 - Spencer Miller.csv\\nWaiters Cash_Sales.csv\\nWhole set\\nWine Inventory - Andrew Nelson.csv\\nWood purchased - Patrick Bernier.csv\\nWoodworking Class Data - Patrick Bernier.csv\\n\",\n",
              " 'stderr': '',\n",
              " 'returncode': 0,\n",
              " 'pid': None}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import terminal\n",
        "terminal.run_command(command='ls Datasets')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SuNZGUSGZpdt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuNZGUSGZpdt",
        "outputId": "8458bbaa-ca59-4518-e4d0-4c482deeec04"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'message': 'Command completed with exit code 0. Workspace state updated.',\n",
              " 'stdout': 'inflation adjustment affects,account code,alternative account code,account name,disabling date ,enabled,enablement date,account ID,ID information,assets,credit,debits,liability ,net loss,net income,credit balance,opening credit balance,debit balance,opening debit balance,usual balance,account type,use accounting assistants,use additional unit\\n',\n",
              " 'stderr': '',\n",
              " 'returncode': 0,\n",
              " 'pid': None}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "terminal.run_command(\n",
        "    command=\"sh -c 'head -n 3 \\\"Datasets/2_Debits_and_Credits_Purchases.csv\\\" | cat'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jp75cqqQFa_m",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jp75cqqQFa_m",
        "outputId": "a3882904-9d52-4ea9-9909-a132467ac40f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'message': 'Command completed with exit code 0. Workspace state updated.',\n",
              " 'stdout': 'No accounts with an initial credit balance\\n',\n",
              " 'stderr': '',\n",
              " 'returncode': 0,\n",
              " 'pid': None}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "terminal.run_command(command='python3 -c \"import pandas as pd; df = pd.read_csv(\\'Datasets/2_Debits_and_Credits_Purchases.csv\\'); \\\n",
        "df[\\'opening credit balance\\'] = pd.to_numeric(df[\\'opening credit balance\\'], errors=\\'coerce\\'); \\\n",
        "result = df.loc[df[\\'opening credit balance\\'] > 0, [\\'account name\\', \\'opening credit balance\\']]; \\\n",
        "print(result if not result.empty else \\'No accounts with an initial credit balance\\')\"')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M5C9NoMrN6Ix",
      "metadata": {
        "id": "M5C9NoMrN6Ix"
      },
      "source": [
        "# Golden Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cT1ofBX4N5uL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cT1ofBX4N5uL",
        "outputId": "b39e325b-164a-4392-d7a7-443ca3a82640"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "there are no accounts with an initial credit balance in the dataset.\n"
          ]
        }
      ],
      "source": [
        "print(\"there are no accounts with an initial credit balance in the dataset.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}